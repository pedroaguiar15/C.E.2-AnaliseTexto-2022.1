---
title: "Análise de Texto - C.E.2"
subtitle: "2022.1 - UnB"
author: "Letícia Lino, Lucas Augusto, Lucas Franca e Pedro Aguiar"
date: "31/08/2022"
toc: true
theme: "AnnArbor"
colortheme: "dolphin"
format: beamer
editor: visual
editor_options: 
  chunk_output_type: inline
---

# Introdução

## Mineração de dados

A mineração de dados (*data mining*) é um processo técnico, automático ou semiautomático, que analisa grandes quantidades de informações dispersas para que tenham sentido e sejam convertidas em conhecimento. Busca anomalias, padrões ou correlações entre milhões de registros para predizer resultados.

## Mineração e análise de texto

O termo mineração de texto (*text mining)* é um pouco menos conhecido, mas a ideia é bastante semelhante. A diferença reside principalmente no tipo de dado analisado. Enquanto a de dados lida mais com bancos de dados, o *text mining* faz essa mineração em dados não estruturados.

A análise de texto combina um conjunto de técnicas de aprendizado de máquina, estatísticas e linguísticas para processar grandes volumes de texto não estruturado ou texto que não tem um formato predefinido (Word e PDF's, por exemplo), para derivar percepções e padrões. Textos extraídos de redes sociais também podem ser processados.

------------------------------------------------------------------------

A mineração de texto e a análise de texto costumam ser usadas de forma intercambiável. O termo mineração de texto é geralmente usado para derivar percepções qualitativas de texto não estruturado, enquanto a análise de texto fornece resultados quantitativos.Por exemplo, a mineração de texto pode ser usada para identificar se os clientes estão satisfeitos com um produto, analisando suas avaliações e pesquisas. A análise de texto é usada para informações mais profundas, como identificar um padrão ou tendência de um texto não estruturado (para entender um aumento negativo na experiência do cliente ou na popularidade de um produto, por exemplo).

------------------------------------------------------------------------

### Técnicas de análise de texto e casos de uso

-   Análise de sentimentos

-   Modelagem de tópicos

-   Reconhecimento de entidade nomeada (NER)

-   Frequência do termo - frequência inversa do documento (TF-IDF)

-   Extração de evento

    -   Análise de links

    -   Análise geoespacial

    -   Monitoramento de riscos de negócios

------------------------------------------------------------------------

### Etapas da análise de texto

1.  Coleta de dados

2.  Preparação de dados

    1.  Tokenização

    2.  Marcação de parte do discurso

    3.  Lematização e origem

    4.  Remoção de palavras irrelevantes

3.  Análise de texto

4.  Visualização\
    \
    \

# Mineração e análise de texto de dados não estruturados com *R*

## O Formato *Tidy Text*

Serão analisados os seguintes artigos científicos:

-   "The Production of Comedy: The Joke in the Age of Social Media" Sturges, Paul

-   "The Strives, Struggles, and Successes of Women Diagnosed With ADHD as Adults" Glaser, Mira; Langvik, Eva

-   "Social Revolutions: Their Causes, Patterns, and Phases" Tiruneh, Gizachew

------------------------------------------------------------------------

Lendo os artigos em PDF no *R*.

```{r, echo=TRUE}

require(pdftools)
require(stringr)
require(dplyr)

humorepiadas <- pdf_text("A piada na era da mídia social - alt.pdf") %>% 
  str_c(collapse = "") %>% 
  str_replace_all(pattern = "\n", replacement = " PAGINA ") %>% 
  str_squish() %>% 
  str_split(pattern = "PAGINA") %>% 
  unlist


```

------------------------------------------------------------------------

```{r, echo=TRUE}

revsociais <- pdf_text("Revoluções Sociais - alt.pdf") %>% 
  str_c(collapse = "") %>% 
  str_replace_all(pattern = "\n", replacement = " PAGINA ") %>% 
  str_squish() %>% 
  str_split(pattern = "PAGINA") %>% 
  unlist

tdahmulheres <- pdf_text("Os esforços, lutas e sucessos de mulheres adultas com TDAH - alt.pdf")%>% 
  str_c(collapse = "") %>% 
  str_replace_all(pattern = "\n", replacement = " PAGINA ") %>% 
  str_squish() %>% 
  str_split(pattern = "PAGINA") %>% 
  unlist

```

------------------------------------------------------------------------

Transformando em um *dataframe* com "***tidytext***"

```{r, echo=TRUE}

require(tidytext)

humorepiada_df <- humorepiadas %>% 
  as.data.frame() %>% 
  mutate(numerolinha = row_number()) %>% 
  rename("texto" = ".") %>% 
  unnest_tokens(word, texto) %>% 
  as_tibble() %>% 
  mutate(Autor = "Sturges")
```

------------------------------------------------------------------------

```{r, echo=FALSE}
humorepiada_df
```

------------------------------------------------------------------------

```{r, echo=TRUE}
revsociais_df <- revsociais %>%
  as.data.frame() %>% 
  mutate(numerolinha = row_number()) %>% 
  rename("texto" = ".") %>% 
  unnest_tokens(word, texto) %>% 
  as_tibble() %>% 
  mutate(Autor = "Tiruneh")
```

------------------------------------------------------------------------

```{r, echo=FALSE}
revsociais_df
```

------------------------------------------------------------------------

```{r, echo=TRUE}
tdahmulheres_df <- tdahmulheres %>% 
  as.data.frame() %>% 
  mutate(numerolinha = row_number()) %>% 
  rename("texto" = ".") %>% 
  unnest_tokens(word, texto) %>% 
  as_tibble() %>% 
  mutate(Autor = "Glaser and Langvik")
```

------------------------------------------------------------------------

```{r, echo=FALSE}
tdahmulheres_df
```

------------------------------------------------------------------------

Removendo palavras "vazias"

```{r, echo=TRUE}
data(stop_words)
humorepiada_df <- humorepiada_df %>% 
  anti_join(stop_words)

data(stop_words)
revsociais_df <- revsociais_df %>% 
  anti_join(stop_words)

data(stop_words)
tdahmulheres_df <- tdahmulheres_df %>% 
  anti_join(stop_words)
```

------------------------------------------------------------------------

Contagem - Humor & Piada

```{r, echo=FALSE}
humorepiada_df %>% count(word, sort = TRUE)
```

------------------------------------------------------------------------

Contagem - Revoluções Sociais

```{r, echo=FALSE}
revsociais_df %>% count(word, sort = TRUE)
```

------------------------------------------------------------------------

Contagem - TDAH em Mulheres

```{r, echo=FALSE}
tdahmulheres_df %>% count(word, sort = TRUE)
```

------------------------------------------------------------------------

Visualização - Humor & Piada

```{r, echo=FALSE}
require(ggplot2)

humorepiada_df %>% 
  count(word, sort = TRUE) %>%
  filter(n > 10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

------------------------------------------------------------------------

Visualização - Revoluções Sociais

```{r, echo=FALSE}
revsociais_df %>% 
  count(word, sort = TRUE) %>%
  filter(n > 30) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

------------------------------------------------------------------------

Visualização - TDAH em Mulheres

```{r, echo=FALSE}
tdahmulheres_df %>% 
  count(word, sort = TRUE) %>%
  filter(n > 20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

## Análise de sentimentos

### Bibliotecas para análise de sentimentos com "*textdata"*

------------------------------------------------------------------------

"*Afinn*"

```{r, echo=TRUE}
require(textdata)
get_sentiments("afinn") 
```

------------------------------------------------------------------------

"*Bing*"

```{r, echo=FALSE}
get_sentiments("bing") 
```

------------------------------------------------------------------------

"*NRC*"

```{r, echo=FALSE}
get_sentiments("nrc")
```

------------------------------------------------------------------------

"*NRC* - Revoluções Sociais - Medo"

```{r, echo=TRUE}
nrc_medo <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")
```

------------------------------------------------------------------------

```{r, echo=FALSE}

revsociais_df %>%
  inner_join(nrc_medo) %>%
  count(word, sort = TRUE)
```

------------------------------------------------------------------------

"*BING* - Humor & Piadas - Positivo"

```{r, echo=FALSE}
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")

humorepiada_df %>% 
  inner_join(bing_pos) %>% 
  count(word, sort = TRUE)
```

------------------------------------------------------------------------

"*AFINN* - TDAH em Mulheres - 3 negativo"

```{r, echo=FALSE}
afinn_tresneg <- get_sentiments("afinn") %>% 
  filter(value == -3)

tdahmulheres_df %>% 
  inner_join(afinn_tresneg) %>% 
  count(word, sort = TRUE)
```

------------------------------------------------------------------------

### Palavras positivas e negativas - Quantidade

------------------------------------------------------------------------

Humor & Piadas

```{r, echo=TRUE}
humorepiada_sent_bing <- humorepiada_df %>% 
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("bing")) #jokes é negativo; não tem comedy;

humorepiada_sent_afinn <- humorepiada_df %>% 
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("afinn")) # jokes é 2; comedy é 1;

humorepiada_sent_nrc <- humorepiada_df %>% 
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("nrc")) # jokes é negativo; não tem comedy;
```

------------------------------------------------------------------------

Revoluções Sociais

```{r, echo=TRUE}
revsociais_sent_bing <- revsociais_df %>% 
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("bing")) #tem revolutionary mas não tem revolution(s); popular é positivo;
# defeat é positivo;

revsociais_sent_afinn <- revsociais_df %>% 
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("afinn")) # não tem revolution - political - social; 
#talvez algumas palavras não tem classificação?

revsociais_sent_nrc <- revsociais_df %>% 
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("nrc")) #revolution tem vários sentimentos # military = fear
#defeat é negativo
```

------------------------------------------------------------------------

TDAH em Mulheres

```{r, echo=TRUE}
tdahmulheres_sent_bing <- tdahmulheres_df %>% 
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("bing")) #symptoms é negativo; lack é negativo; 
#a maioria das palavras mais frequentes não aparecem também

tdahmulheres_sent_afinn <- tdahmulheres_df %>% 
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("afinn")) #as 10 palavras mais frequentes não aparecem (neutralidade)?;
#positive é 2 e negative é -2; 

tdahmulheres_sent_nrc <- tdahmulheres_df %>% 
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("nrc")) #multisentimentalidade; esteem = alegria, positivo e tristeza;
```

------------------------------------------------------------------------

### Visualização

*Bing* - Humor & Piadas

------------------------------------------------------------------------

```{r, echo=FALSE}
humorepiadas_maisfreqs_neg <- filter(humorepiada_sent_bing, `sentiment` == "negative")

ggplot(humorepiadas_maisfreqs_neg[1:20,], aes(x = reorder(word, n), n)) +
  geom_col() +
  facet_wrap(~sentiment) +
  labs(x = "Palavra", y = "Quantidade", title = "Humor & Piadas", 
       subtitle = "BING - NEGATIVE") + 
  coord_flip()
```

------------------------------------------------------------------------

```{r, echo=FALSE}
humorepiadas_maisfreqs_pos <- filter(humorepiada_sent_bing, `sentiment` == "positive")

ggplot(humorepiadas_maisfreqs_pos[1:20,], aes(x = reorder(word, n), n)) +
  geom_col() +
  facet_wrap(~sentiment) +
  labs(x = "Palavra", y = "Quantidade", title = "Humor & Piadas", 
       subtitle = "BING - POSITIVE") + 
  coord_flip()

```

------------------------------------------------------------------------

```{r, echo=FALSE}
ggplot(humorepiada_sent_bing[1:20,], aes(x = reorder(word, n), n)) +
  geom_col(aes(fill = sentiment)) +
  labs(x = "Palavra", y = "Quantidade", title = "Humor & Piadas", 
       subtitle = "BING") +
  coord_flip()
```

------------------------------------------------------------------------

*BING* - Revoluções Sociais

------------------------------------------------------------------------

```{r, echo=FALSE}
revsociais_maisfreqs_neg <- filter(revsociais_sent_bing, `sentiment` == "negative")

ggplot(revsociais_maisfreqs_neg[1:20,], aes(x = reorder(word, n), n)) +
  geom_col() +
  facet_wrap(~sentiment) +
  labs(x = "Palavra", y = "Quantidade", title = "Revoluções Sociais", 
       subtitle = "BING - NEGATIVE") + 
  coord_flip()
```

------------------------------------------------------------------------

```{r, echo=FALSE}
revsociais_maisfreqs_pos <- filter(revsociais_sent_bing, `sentiment` == "positive")

ggplot(revsociais_maisfreqs_pos[1:20,], aes(x = reorder(word, n), n)) +
  geom_col() +
  facet_wrap(~sentiment) +
  labs(x = "Palavra", y = "Quantidade", title = "Revoluções Sociais", 
       subtitle = "BING - POSITIVE") + 
  coord_flip()
```

------------------------------------------------------------------------

```{r, echo=FALSE}
ggplot(revsociais_sent_bing[1:20,], aes(x = reorder(word, n), n)) +
  geom_col(aes(fill = sentiment)) +
  labs(x = "Palavra", y = "Quantidade", title = "Revoluções Sociais", 
       subtitle = "BING") +
  coord_flip()
```

------------------------------------------------------------------------

*BING* - TDAH em Mulheres

------------------------------------------------------------------------

```{r, echo=FALSE}
tdahmulheres_maisfreqs_neg <- filter(tdahmulheres_sent_bing, `sentiment` == "negative")

ggplot(tdahmulheres_maisfreqs_neg[1:20,], aes(x = reorder(word, n), n)) +
  geom_col() +
  facet_wrap(~sentiment) +
  labs(x = "Palavra", y = "Quantidade", title = "TDAH & Mulheres", 
       subtitle = "BING - Negative") + 
  coord_flip()
```

------------------------------------------------------------------------

```{r, echo=FALSE}
tdahmulheres_maisfreqs_pos <- filter(tdahmulheres_sent_bing, `sentiment` == "positive")

ggplot(tdahmulheres_maisfreqs_pos[1:20,], aes(x = reorder(word, n), n)) +
  geom_col() +
  facet_wrap(~sentiment) +
  labs(x = "Palavra", y = "Quantidade", title = "Humor & Piadas", 
       subtitle = "BING - POSITIVE") + 
  coord_flip()
```

------------------------------------------------------------------------

```{r}
ggplot(tdahmulheres_sent_bing[1:20,], aes(x = reorder(word, n), n)) +
  geom_col(aes(fill = sentiment)) +
  labs(x = "Palavra", y = "Quantidade", title = "Humor & Piadas", 
       subtitle = "BING") +
  coord_flip()
```

------------------------------------------------------------------------

### Nuvens de palavras

*Wordcloud* - Humor & Piadas

```{r, echo=FALSE}
require(wordcloud)
require(reshape2)
humorepiada_df %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))
```

------------------------------------------------------------------------

*Wordcloud* - Revoluções Sociais

```{r, echo=FALSE}
revsociais_df %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))

```

------------------------------------------------------------------------

*Wordcloud* - TDAH em Mulheres

```{r, echo=FALSE}
tdahmulheres_df %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))

```

------------------------------------------------------------------------

```{r, echo=FALSE}
humorepiada_df %>% 
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "blue"),
                   max.words = 100)
```

------------------------------------------------------------------------






###FIM PARTE PEDRO !!! -----------------------------------------------------------------------------------------------------------------------------------------------






## Ponderação de termos

-   Parte da ideia de que, dentro de um contexto, alguns termos podem ser mais importantes do que outros para descrever o conteúdo dos documentos;

-   Por exemplo, um termo que apareça em todos os documentos da base não tem muita utilidade para indexação. Por outro lado, um termo raro pode ter grande importância;

-   Por sua vez, um termo que apareça muitas vezes em um documento específico pode, em muitos casos, dar uma ideia melhor sobre o conteúdo desse documento do que um termo que apareça poucas vezes.

------------------------------------------------------------------------

## Document-term matrix (DTM)

-   Todos os termos de um conjunto de documentos são colocados em uma DTM,

-   Uma DTM é uma matriz matemática que descreve a frequência dos termos que ocorrem em uma coleção de documentos.

-   Linhas: Termos

-   Colunas: Documentos

-   Esquema de ponderação de termos: Term Frequency, Binary Weight, TF-IDF, etc.

------------------------------------------------------------------------

### Um exemplo de DTM

-   Doc1: I like R
-   Doc2: I like Python

| Termos | Doc1 | Doc2 |
|--------|------|------|
| I      | 1    | 1    |
| Like   | 1    | 1    |
| Python | 0    | 1    |
| R      | 1    | 0    |

------------------------------------------------------------------------

### No R: Criando uma DTM

-   Podemos usar o pacote `tm` que contém as seguintes funções:

-   A função `VectorSource()` interpreta os elementos de um vetor $x$ como documentos;

-   A função `Corpus()` cria um objeto do tipo corpus;

-   A função `TermDocumentMatrix()` recebe um objeto do tipo corpus como argumento, e cria uma DTM;

-   A função `Inspect()` apresenta informações detalhadas sobre uma DTM;

------------------------------------------------------------------------

-   Exemplo:

```{r, echo = T}
library(magrittr)
library(tm) ## package for text mining
```

------------------------------------------------------------------------

```{r, echo=FALSE}
a <- c("I like R", "I like Python")
## build corpus
b <- a %>% tm::VectorSource() %>% tm::Corpus()
## build term document matrix (TDM)
m <- b %>% tm::TermDocumentMatrix(control=list(wordLengths=c(1, Inf)))
m %>% tm::inspect()
```

------------------------------------------------------------------------

## No R: Diferentes esquemas de Ponderação (binary weighting)

-   Simples esquema de ponderação por binários

-   Se o termo está no documento é atribuído o valor de $1$, se ele não está no documento é atribuído o valor de $0$.

------------------------------------------------------------------------

-   Exemplo:

```{r, echo = T}
## various term weighting schemes
m %>% weightBin() %>% inspect() ## binary weighting
```

------------------------------------------------------------------------

-   Se baseia na premissa de que quanto mais vezes um termo aparece em um documento, maior sua capacidade de descrever seu conteúdo.

-   Assim, o peso do termo no documento é proporcional a sua frequência.

------------------------------------------------------------------------

-   Exemplo:

```{r, echo = T}
m %>% weightTf() %>% inspect() ## term frequency
```

------------------------------------------------------------------------

## No R: Diferentes esquemas de Ponderação (TF-IDF)

-   TF-IDF é uma medida estatística que tem o intuito de indicar a importância de uma palavra de um documento em relação a uma coleção de documentos.

-   Term Frequency (TF): é o número de ocorrências do termo $t_i$ no documento $d_j$

-   Inverse Document Frequency (IDF) é um peso atribuído para cada termo para medir seu grau de importância em relação à uma coleção de documentos de texto.

------------------------------------------------------------------------

-   O IDF de um termo $t$ é definido como:

$$
idf(term) = ln(\frac{n_{documents}}{n_{documents\: containing \:term}}) 
$$

$$
tfidf(term) = tf_{ij} \: . \: idf_{term}
$$

------------------------------------------------------------------------

-   O IDF reduz o peso de termos que ocorrem com frequência em documentos e aumenta o peso de termos que ocorrem raramente.

-   Busca expressar a importância de um termo dentro da base de documentos segundo sua raridade;

-   TD-IDF é o esquema de ponderação mais popular na prática;

------------------------------------------------------------------------

-   No R, podemos usar a função `weightTfIdf()`:

```{r, echo = T}
m %>% weightTfIdf(normalize=F) %>% inspect() 
```

------------------------------------------------------------------------

### No R: Diferentes esquemas de Ponderação (normalized TF-IDF)

-   A normalização é usada para evitar viés na frequência de termos em documentos mais curtos ou mais longos.

```{r, echo = T}
m %>% weightTfIdf(normalize=T) %>% inspect() 
```

------------------------------------------------------------------------







###FIM PARTE LUCAS COELHO --------------------------------------------------------------------------------------------------------------------------------------------








## Relações entre palavras

**Até agora...**

-   Palavras como unidades individuais;

-   Relações com sentimentos ou documentos.

**No entanto...**

-   Relações entre palavras;

-   Quais palavras tendem a seguir outras imediatamente;

-   Co-ocorrer dentro dos mesmos documentos.

**Ou seja...** Exploraremos alguns dos métodos para calcular e visualizar relacionamentos entre palavras em seu conjunto de dados de texto.

## Tokenização por n-grama

**O que é um n-grama?** - Um n-grama é uma sequência contínua de *n* itens de uma determinada amostra de texto.

A função ***unnest_tokens()*** também pode ser utilizada para tokenizar sequências consecutivas de palavras, ou seja, n-gramas.

Podemos ver com que frequência a palavra X é seguida pela palavra Y, através um modelo de relação entre ambas.

## Tokenização por n-grama

Para a demosntração, uilizaremos os seguintes pacotes: 

```{r echo=TRUE}
library(dplyr)
library(tidytext)
library(pdftools)
library(stringr)
library(ggplot2)
library(tm)
library(wordcloud)
library(tidyverse)
library(ggraph)
library(igraph)
```

E o livro "*Dom Casmurro*", de Machado de Assis, como texto para análise.

```{r echo=F}
library(tidyverse)
# Importando Texto
texto<-pdf_text('Dom_Casmurro.pdf')
# Limpando Texto
dom_casmurro1<-texto[3:337] %>%
  strsplit("\n") %>% 
  unlist() %>% 
  str_squish() %>% #Retirando os múltiplos espaços
  str_replace_all('cAPÍTULo', 'CAPÍTULO')%>%
  str_to_lower() %>%
  enframe(name = NULL, value = "linha")

dom_casmurro <- dom_casmurro1 %>%
  mutate(capitulo = cumsum(str_detect(linha, # descobrir onde estão os capítulos
                                     regex("capítulo",
                                           ignore_case = FALSE)))) %>%
  ungroup()
```

## Tokenização por n-grama

**Trasformando cada Token = n-gramas**

O ***bigram***, utilizado no código abaixo, representa a definição de n = 2, ou seja, estamos examinando pares de duas palavras consecutivas. 
```{r echo=TRUE}
bigram_dc <- dom_casmurro %>%
  unnest_tokens(bigram, linha, token = "ngrams", n = 2)
```
Para examinarmos 3 palavras consecutivas, utilizamos o ***trigram*** no lugar do ***bigram***, e mudamos o n = 3.
```{r echo=TRUE}
trigram_dc <- dom_casmurro %>%
  unnest_tokens(trigram, linha, token = "ngrams", n = 3)
```

## Tokenização por n-grama

O resultado fica assim para n = 2, onde cada token representa um bigrama: 

```{r echo=T}
bigram_dc
```

## Tokenização por n-grama

O resultado fica assim para n = 3, onde cada token representa um trigrama: 

```{r echo=T}
trigram_dc
```

## Tokenização por n-grama

***Podemos contar e filtrar n-gramas*** 

```{r echo=T}
bigram_dc %>% count(bigram, sort = T)
```

## Tokenização por n-grama

Muitos dos bigramas mais comuns são pares de palavras comuns (stopwords), como "*que a*", "*que o*", e entre outros. 
Para resolvermos esse problema, utilizaremos a função ***separate()*** e ***filter()***. Vale ressaltar que o *stopwords* é do pacote ***tm***.

**Separando**
```{r echo=T}
library(tm)
# Separando o Bigramas
bigrams_separados <- bigram_dc %>%
  separate(bigram, c("palavra1", "palavra2"), sep = " ")
bigrams_separados
```

## Tokenização por n-grama
**Filtrando**
```{r echo=T}
# Filtrando o Bigramas
filtrando_bigramas <- bigrams_separados%>%
  filter(!palavra1 %in% stopwords('pt')) %>%
  filter(!palavra2 %in% stopwords('pt'))
filtrando_bigramas
```

## Tokenização por n-grama
**Nova contagem**
```{r echo=T}
# Criando uma nova contagem dos Bigramas
novo_bigramas_dc <- filtrando_bigramas %>% 
  count(palavra1, palavra2, sort = TRUE)
```

## Tokenização por n-grama

Por fim, após removermos as stopwords, temos o seguinte resultado: 

```{r echo=T} 
novo_bigramas_dc
```

## Tokenização por n-grama

Para recombinar, basta utilizar a função ***unite()***:

```{r echo=T}
bigramas_dc_unido <- filtrando_bigramas %>%
  unite(bigram, palavra1, palavra2, sep = " ")
bigramas_dc_unido
```

## Visualizando uma rede de bigramas com ggraph e igraph

Podemos visualizar todas as relações entre as palavras simultaneamente, em vez de apenas as primeiras de cada vez.\
**Como?**
Podemos organizar as palavras em uma rede, ou “gráfico”.
O pacote **igraph** tem funções que auxiliam a manipulação e a análise de redes.\
Podemos utilizar a função ***graph_from_data_frame()*** para criar um objeto igraph a partir de dados arrumados. Essa função recebe um quadro de dados de arestas com colunas atributos **“from”**, **“to"** e ***"weight"*** (neste caso n).\ 

- **from** : o nó de onde uma aresta está vindo
- **to** : o nó para o qual uma aresta está indo
- **weight** : Um valor numérico associado a cada aresta

## Visualizando uma rede de bigramas com ggraph e igraph
**Gerando uma rede utilizando o** ***igraph***
```{r echo=T}
library(igraph)
rede_bigrama_dc <- novo_bigramas_dc %>%
  filter(n > 5) %>%
  graph_from_data_frame()
rede_bigrama_dc
```

## Visualizando uma rede de bigramas com ggraph e igraph

O igraph tem funções de plotagem embutidas, mas não é ele que reproduz a visualização. 
Para isso, podemos converter um objeto igraph em um **ggraph** com a função **ggraph()**, após adicionarmos camadas a ele, o transmitiremos ao **ggplot2**.

```{r echo=T}
library(ggraph)
set.seed(2017)
grafico <- ggraph(rede_bigrama_dc, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

## Visualizando uma rede de bigramas com ggraph

```{r echo=F}
grafico
```

## Aplicação interessante...

```{r echo=F, out.width = '80%', fig.pos="center"}
if(require(devtools) == F) install.packages('devtools'); 
require(devtools);
devtools::install_github("davi-moreira/txt4cs-pkg")
require(txt4cs)
library(txt4cs)
# Transformando as falas do impeachment em bigramas
data("impeachment-dilma")
impeachment <- impeachmentDilma %>%
  unnest_tokens(bigram, discursoPlainTxt, token = "ngrams", n = 2)
# Separando os bigramas
separando_bigram <- impeachment %>%
  separate(bigram, c("word1", "word2"), sep = " ")
# Filtrando os bigramas
filtrando_bigram <- separando_bigram %>%
  filter(!word1 %in% stopwords("pt")) %>%
  filter(!word2 %in% stopwords("pt"))
# Contado os bigramas
bigram_counts <- filtrando_bigram %>%
  count(word1, word2, sort = TRUE)
# Montando uma rede
bigram_graph <- bigram_counts %>%
  filter(n > 15) %>%
  graph_from_data_frame()
set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

## Aplicação interessante...

```{r echo=T, out.width = '30%', fig.pos="center"}
library(wordcloud)
bigram_unido <- filtrando_bigram %>%
  unite(bigram, word1, word2, sep = " ")
bigram_unido %>%
  count(bigram, sort = T) %>%
  with(wordcloud(bigram, n, max.words = 25, colors=brewer.pal(8, "Dark2")))
```

## Aplicação interessante...

```{r echo=FALSE, out.width = '70%', fig.pos="center"}
if(require(devtools) == F) install.packages('devtools'); 
require(devtools);
devtools::install_github("davi-moreira/txt4cs-pkg")
require(txt4cs)
library(txt4cs)

# Transformando as falas do impeachment em bigramas
data("impeachment-dilma")
impeachment<-impeachmentDilma
impeachmentPT <- subset(impeachment,impeachment$paritdo=='PT')
impeachmentPMDB <- subset(impeachment, impeachment$paritdo=='PMDB')
novo_impeachment<-full_join(impeachmentPMDB,impeachmentPT)

# Imp do PT + PMDB ----
# Transformando as falas do impeachment em bigramas
impeachment_rede <- novo_impeachment %>%
  unnest_tokens(bigram, discursoPlainTxt, token = "ngrams", n = 2)
# Separando os bigramas
imp_sep_bigram <- impeachment_rede %>%
  separate(bigram, c("word1", "word2"), sep = " ")
# Filtrando os bigramas
filtrando_imp_bigram <- imp_sep_bigram %>%
  filter(!word1 %in% stopwords("pt")) %>%
  filter(!word2 %in% stopwords("pt"))
# Contado os bigramas
bigram_imp_counts <- filtrando_imp_bigram %>%
  count(word1, word2, sort = TRUE)
# Montando uma rede
bigram_imp_graph <- bigram_imp_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()
set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

imppmdbpt<-ggraph(bigram_imp_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "green", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

# Imp PT ----
# Transformando as falas do impeachment em bigramas
impeachment_pt <- impeachmentPT %>%
  unnest_tokens(bigram, discursoPlainTxt, token = "ngrams", n = 2)
# Separando os bigramas
pt_sep_bigram <- impeachment_pt %>%
  separate(bigram, c("word1", "word2"), sep = " ")
# Filtrando os bigramas
filtrando_pt_bigram <- pt_sep_bigram %>%
  filter(!word1 %in% stopwords("pt")) %>%
  filter(!word2 %in% stopwords("pt"))
# Contado os bigramas
bigram_pt_counts <- filtrando_pt_bigram %>%
  count(word1, word2, sort = TRUE)
# Montando uma rede
bigram_pt_graph <- bigram_pt_counts %>%
  filter(n > 5) %>%
  graph_from_data_frame()
set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

pt<-ggraph(bigram_pt_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "red", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

# Imp PMDB ----
# Transformando as falas do impeachment em bigramas
impeachment_pmdb <- impeachmentPMDB %>%
  unnest_tokens(bigram, discursoPlainTxt, token = "ngrams", n = 2)
# Separando os bigramas
pmdb_sep_bigram <- impeachment_pmdb %>%
  separate(bigram, c("word1", "word2"), sep = " ")
# Filtrando os bigramas
filtrando_pmdb_bigram <- pmdb_sep_bigram %>%
  filter(!word1 %in% stopwords("pt")) %>%
  filter(!word2 %in% stopwords("pt"))
# Contado os bigramas
bigram_pmdb_counts <- filtrando_pmdb_bigram %>%
  count(word1, word2, sort = TRUE)
# Montando uma rede
bigram_pmdb_graph <- bigram_pmdb_counts %>%
  filter(n > 5) %>%
  graph_from_data_frame()
set.seed(2020)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

pmdb<-ggraph(bigram_pmdb_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

imppmdbpt
```

## Aplicação interessante ...
```{r echo=FALSE, out.width = '70%', fig.pos="center"}
pt
```

## Aplicação interessante ...
```{r echo=FALSE, out.width = '70%', fig.pos="center"}
pmdb
```






###FIM PARTE LUCAS AUGUSTO -------------------------------------------------------------------------------------------------------------------------------------------







-   "Text Mining with R" - Julia Silge & David Robinson

    -   [tidytextmining.com](https://www.tidytextmining.com/index.html)

-   <https://www.tibco.com/pt-br/reference-center/what-is-text-analytics>

Dom Casmurro - Machado de Assis

String manipulation with stringr : : CHEAT SHEET
